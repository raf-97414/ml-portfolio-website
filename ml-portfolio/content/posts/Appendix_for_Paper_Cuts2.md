---
title: "Appendix Section of the Paper: Discovering Physical Concepts With Neural Networks Simplified ğŸ¤–âœ¨"
date: 2025-01-11
draft: false
tags: ["AI", "Neural Networks", "VAEs", "Machine Learning"]
categories: ["Learning"]
---

## ğŸ¤– Appendix A: Neural Networks 101

### **Whatâ€™s a Neural Network?**
A neural network is like a group of friends ğŸ‘« working together. Each friend (a **neuron**) takes some inputs (e.g., gossip) and decides whatâ€™s important before passing it along. ğŸ“¤

---

### **1. Single Artificial Neuron: The Basic Unit** âš™ï¸
Imagine one neuron is a super-smart *calculator friend*. ğŸ§® Hereâ€™s what it does:

1. Takes inputs ... ğŸ–Šï¸  
2. Multiplies them by *weights* ... ğŸ’ª  
3. Adds a *bias* b , which is like its mood. ğŸŒ€  
4. Passes the result through an **activation function**, which decides how excited it gets. ğŸ‰  

#### **Activation Function**  
For this paper, they used **Exponential Linear Unit (ELU)**:
- If the input is positive: ğŸŸ¢ **Keep it as it is**.  
- If the input is negative: ğŸ”´ **Smooth it out** using a fancy exponential formula. ğŸ§™

---

### **2. Neural Networks: Like a Super Social Network** ğŸŒ
When you connect lots of neurons together, you get a **neural network**. ğŸ›ï¸ Here's the setup:

1. **Input Layer**: Where data comes in (e.g., numbers, images). ğŸ‘€  
2. **Hidden Layers**: Where the neurons gossip and figure out patterns. ğŸ¤«  
3. **Output Layer**: Where decisions or answers are made (e.g., "cat or dog?"). ğŸ±ğŸ¶  

Neural networks are **universal approximators**, meaning they can learn almost anythingâ€”like a genius who just needs enough practice. ğŸ¤“ğŸ¯

---

### **3. Training: How Neural Networks Learn** ğŸ“ğŸ§‘â€ğŸ«

Neural networks are lazy at first (all weights and biases are random). ğŸ² But during training, they adjust themselves to get better at solving problems. Hereâ€™s how:

1. **Cost Function**: Measures how bad the network's predictions are (like a test score ğŸ“‰).  
2. **Gradient Descent**: The network uses feedback to adjust weights and biases, slowly improving. ğŸ’ª  
   - This is like hiking downhill to find the lowest point of a valley. â›°ï¸â¬‡ï¸  
3. **Stochastic Gradient Descent**: Instead of using the entire dataset at once, it trains in small batches (mini-study sessions). ğŸ“š  

They also use **backpropagation**, which is like a teacher correcting the network's mistakes step by step. ğŸ§‘â€ğŸ«ğŸ”„

---

## ğŸ¤” Appendix B: Variational Autoencoders (VAEs)

### **Whatâ€™s a VAE?**
A **Variational Autoencoder** is like a super-organized librarian. ğŸ“š It:
1. **Encodes** high-dimensional data (e.g., a huge book) into a **compact summary** (e.g., a sticky note). ğŸ—’ï¸  
2. **Decodes** the summary back into the full data, ensuring it makes sense. ğŸ”„  

---

### **1. Representation Learning: Why Compress Data?** ğŸ—œï¸
The goal is to turn complex input x  into a smaller representation z  while keeping all the important stuff.  
Think of it like summarizing "War and Peace" into a tweet. ğŸ¦

- **Encoder**: Maps input x  â†’ representation z .  
- **Decoder**: Uses z to recreate x .  

---

### **2. Probabilistic Encoder/Decoder: Adding Uncertainty** ğŸ²
Instead of mapping x to z deterministically, VAEs use probabilities. ğŸ¤”  
- The encoder says: â€œHmm, z could be anywhere in this range.â€ ğŸ¯  
- The decoder takes these probabilities and guesses the original data. ğŸ”„  

To make this work, we assume z follows a normal (Gaussian) distribution, which makes the math nice and smooth. ğŸ“Š

---

### **3. Reparameterization Trick: Solving the Math Drama** ğŸ§™â€â™‚ï¸
**Problem**: Probabilities are tricky to differentiate. ğŸ˜¤  
**Solution**: **Reparameterization Trick**:
1. Use random noise epsilon from a standard distribution. ğŸ²  
2. Transform it into the required distribution:  
    z = mu + sigma * epsilon 

Now, we can train the network like normal! ğŸš€

---

### **4. The beta VAE Cost Function: Keep it Organized!** ğŸ§¹âœ¨
To train a VAE, we minimize a **cost function**:
1. **Reconstruction Loss**: How well the decoder recreates the input. ğŸ§©  
2. **Regularization**: Encourages disentangled, independent variables in z .  

This makes z super clean, like Marie Kondo organizing a messy house. ğŸ§¹ğŸ 

---

## ğŸ” Appendix C: Interpreting Latent Variables

### **Whatâ€™s a Latent Variable?**
Latent variables are like hidden clues in a mystery novel. ğŸ”  
In SciNet, they represent the **degrees of freedom** needed to describe the system.

**Example**: For a pendulum, the latent variables might be the damping factor and spring constantâ€”nothing else matters! ğŸ‹ï¸

---

## ğŸ”„ Appendix D: Cyclic Representations

### **Whatâ€™s the Problem?**
Some data, like angles, is **cyclic** (e.g., 0 degrees= 360 degrees).  
Neural networks struggle with cyclic data because they assume all data is continuous. ğŸ“ˆ

**Example**:
- Imagine mapping angles on a circle to a line. At 360 degrees, the line suddenly "jumps" back to 0 degrees.  
- This discontinuity makes the network go, â€œWhat the heck?!â€ ğŸ¤¯

---

### **Real-Life Example: The Bloch Sphere in Quantum Mechanics** âš›ï¸
The **Bloch sphere** is used to represent qubits, but itâ€™s cyclic:
**Latitude (theta)** ranges from 0 degrees at the equator to 180 degrees at the poles.
**Longitude (phi)** ranges from 0 degrees to 360 degrees, representing a full circle around the sphere.
 

Neural networks canâ€™t handle the "wrap-around" behavior easily, so they approximate it with steep curves, which leads to errors. ğŸš¨

---

### **In Summary:**
1. Neural networks = smart friends solving problems together. ğŸ¤–  
2. VAEs = librarians compressing data without losing meaning. ğŸ“š  
3. Cyclic data = troublemakers for neural networks. ğŸ”„ğŸŒ€  

